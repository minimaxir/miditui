# Agentic Coding Notes

The app was built using Claude Code, using zero agentic coding shenanigans: just typical prompts and an elaborate [CLAUDE.md](CLAUDE.md), which I have included in this repo. Additionally, I have also included the prompts I gave to Claude Opus 4.5 to help build—[~40 ticket-esque prompts](PROMPTS.md) total before public release—where after each prompt I review the code, manually test, and manually commit if it works as specified.

To summarize those prompts and other relevant findings around agentic coding discovered during this process:

- Opus 4.5 fulfilled the functional requirements on the first attempt with a UI that could indeed compose mix, and play the MIDI audio, but file I/O was nonexistent and some UI/UX decisions were poor with the app being very hard to use, so I took some prompts to polish it. The demo [posted on X](https://x.com/minimaxir/status/2005779586676842646) was after 18 prompts, or halfway through development.
- Mouse usage and window scrolling—and the functional constraints both of them imply—is apparently not easy for LLMs to intuit and consider with changes. For `ratatui`/`crossterm`, it needs to keep in mind an scrolling offset to ensure the clicks are in the right spot when implementing a new feature, which it forgot several times. In hindsight, both of these workflows are less common with TUIs, so now I know to assert them when working with TUIs for agents. A solution for the future would be to establish hints in the `CLAUDE.md` to be aware of these constraints when developing TUIs.
- Another weakness is that out-of-the-box, Claude Code cannot test or build tests for a complex TUI. This resulted in some counterintuitive behaviors such as allowing using keyboard keys as a piano but not allowing the user to press the same key more than once, which matched the prompt instructions _as written_ but doesn't conform to modern user expectations. It's a reminder that LLMs cannot completely replace humans and human testing. In all cases where a UI/UX bug was explicitly identified, Claude Code quickly one-shotted the fix which made it such that it did not significantly impede development.
- I attempted to have Opus 4.5 create a WebAssembly/WASM version of the TUI just to see what would happen. It worked, although the SoundFont sounded completely different and it was not worth the significant dev time to figure out how to debug it. Incidentially, one of Opus 4.5's weaknesses is that it's one of the few top LLMs that cannot take in audio input, so it wouldn't be too much help for debugging this particular issue.

As a former Software QA Engineer of 5 years, these types of bugs are very similar to ones I have seen humans engineers merge in PRs to the `master` branch before handing off to QA time and time again. I do not believe that the bugs are inherently due to LLM agents being better or worse than humans—humans are most definitely capable of making the same mistakes. Even though I myself am adept at finding the bugs and offering solutions, I don't believe that I would inherently avoid _causing_ similar bugs were I to code such an interactive app without AI assistance: QA brain is different from software engineering brain.

Additionally, some have criticized the demo of `miditui` because (paraphrased) "this isn't impressive because MIDI mixers already exist and therefore Claude likely plagiarized this app". I did substantial research and could find zero prior art into how Opus 4.5 could have plagiarized the app and/or its design:

- MIDI synthesizers exist in Rust (`miditui` uses the `rustysynth` [crate](https://crates.io/crates/rustysynth)), but I could not find a public project that creates a UI for MIDI mixing in Rust, terminal-based or otherwise.
- The terminal UI layoyut doesn't resemble any modern MIDI mixer application such as [MuseScore](https://musescore.org/en). In fact, it does the opposite: the timeline for `miditui` is horizontal while most MIDI mixers use a vertical timeline and/or piano sheet music to better match the piano skeuomorphism.
- One reason I am including `PROMPTS.md` for this project is to explicitly show specific prompts _and intents_ for many UI/UX decisions. I am neither a trained UI/UX designer nor a trained audio engineer so these decisions will differ from standard practices—I'm the weirdo who prefers horizontal timelines.
